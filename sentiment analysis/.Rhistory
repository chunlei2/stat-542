train.id = sample(nrow(ratings), floor(nrow(ratings)) * 0.6)
train = ratings[train.id, ]
test = ratings[-train.id, ]
test.id = sample(nrow(test), floor(nrow(test)) * 0.5)
test = test[test.id, ]
label = test[c('user', 'rating')]
test$rating = NULL
#Method 1, UBCF
R = acast(train, user ~ movie)
R = as(R, 'realRatingMatrix')
rec = Recommender(R, method = 'UBCF',
parameter = list(normalize = 'Z-score', method = 'Cosine', nn = 5)
)
recom = predict(rec, R, type = 'ratings')
rec_list = as(recom, 'list')
test$rating = NA
best_rmse = Inf
best_thres = NA
for (thres in seq(1, 4, length.out = 10)) {
for (u in 1:nrow(test)){
# Read userid and movieid from columns 2 and 3 of test data
userid = as.character(test$user[u])
movieid = as.character(test$movie[u])
rating = rec_list[[userid]][movieid]
# 2.5 may be too arbitrary
test$rating[u] = ifelse(is.na(rating), thres, rating)
}
rmse = mean((test$rating - label$rating)^2)
if (rmse < best_rmse) {
best_rmse = rmse
best_thres = thres
}
}
# for (u in 1:nrow(test)){
#
#   # Read userid and movieid from columns 2 and 3 of test data
#   userid = as.character(test$user[u])
#   movieid = as.character(test$movie[u])
#
#   rating = rec_list[[userid]][movieid]
#   # 2.5 may be too arbitrary
#   test$rating[u] = ifelse(is.na(rating), 2.5, rating)
#
# }
# write submission file
# write.table(test, file = 'mysubmission1.csv', row.names = FALSE,
#             col.names = TRUE, sep = ',')
# #Method 1 error
#
# res = merge(test, y, by.x = 'ID')
# rmse = mean((res$Rating - res$rating)^2)
# rmse
best_rmse
best_thres
#Method 1, UBCF
R = acast(train, UserID ~ MovieID)
#Method 1, UBCF
R = acast(train, user ~ movie)
R = as(R, 'realRatingMatrix')
rec = Recommender(R, method = 'POPULAR')
recom = predict(rec, R, type = 'ratings')
rec_list = as(recom, 'list')
test$rating = NA
for (u in 1:nrow(test)){
# Read userid and movieid from columns 2 and 3 of test data
userid = as.character(test$user[u])
movieid = as.character(test$movie[u])
rating = rec_list[[userid]][movieid]
# 2.5 may be too arbitrary
test$rating[u] = ifelse(is.na(rating), 2.5, rating)
}
#Method 1 error
rmse = mean((test$rating - label$rating)^2)
rmse
recommenderRegistry$get_entry('POPULAR')
recommenderRegistry$get_entry_names()
recommenderRegistry$get_entry('POPULAR_realRatingMatrix')
recommenderRegistry$get_entry_names()
recommenderRegistry$get_entries()
#Method 2, Popularity
R = acast(train, user ~ movie)
R = as(R, 'realRatingMatrix')
rec = Recommender(R, method = 'RANDOM')
recom = predict(rec, R, type = 'ratings')
rec_list = as(recom, 'list')
test$rating = NA
for (u in 1:nrow(test)){
# Read userid and movieid from columns 2 and 3 of test data
userid = as.character(test$user[u])
movieid = as.character(test$movie[u])
rating = rec_list[[userid]][movieid]
# 2.5 may be too arbitrary
test$rating[u] = ifelse(is.na(rating), 2.5, rating)
}
dim(recom)
rec_list[[1]]
test$user
test$rating
rec_list[[as.character(135)]]
rec_list
userid
rec_list
rec_list[[userid]]
rec_list[1:5]
rec_list[1:2]
recommenderRegistry$get_entries()
#Method 2, Popularity
R = acast(train, user ~ movie)
R = as(R, 'realRatingMatrix')
rec = Recommender(R, method = 'SVD')
recom = predict(rec, R, type = 'ratings')
rec_list = as(recom, 'list')
test$rating = NA
for (u in 1:nrow(test)){
# Read userid and movieid from columns 2 and 3 of test data
userid = as.character(test$user[u])
movieid = as.character(test$movie[u])
rating = rec_list[[userid]][movieid]
# 2.5 may be too arbitrary
test$rating[u] = ifelse(is.na(rating), 2.5, rating)
}
#Method 1 error
rmse = mean((test$rating - label$rating)^2)
rmse
#Method 2, Popularity
R = acast(train, user ~ movie)
R = as(R, 'realRatingMatrix')
rec = Recommender(R, method = 'SVDF')
recom = predict(rec, R, type = 'ratings')
rec_list = as(recom, 'list')
test$rating = NA
for (u in 1:nrow(test)){
# Read userid and movieid from columns 2 and 3 of test data
userid = as.character(test$user[u])
movieid = as.character(test$movie[u])
rating = rec_list[[userid]][movieid]
# 2.5 may be too arbitrary
test$rating[u] = ifelse(is.na(rating), 2.5, rating)
}
recommenderRegistry$get_entries()
#Method 2, Popularity
R = acast(train, user ~ movie)
R = as(R, 'realRatingMatrix')
rec = Recommender(R, method = 'POPULAR')
recom = predict(rec, R, type = 'ratings')
rec_list = as(recom, 'list')
test$rating = NA
for (u in 1:nrow(test)){
# Read userid and movieid from columns 2 and 3 of test data
userid = as.character(test$user[u])
movieid = as.character(test$movie[u])
rating = rec_list[[userid]][movieid]
# 2.5 may be too arbitrary
test$rating[u] = ifelse(is.na(rating), 2.5, rating)
}
#Method 1 error
rmse = mean((test$rating - label$rating)^2)
rmse
#Method 2, Popularity
R = acast(train, user ~ movie)
R = as(R, 'realRatingMatrix')
rec = Recommender(R, method = 'POPULAR', parameter = list(normalize = 'Z-score'))
recom = predict(rec, R, type = 'ratings')
rec_list = as(recom, 'list')
test$rating = NA
for (u in 1:nrow(test)){
# Read userid and movieid from columns 2 and 3 of test data
userid = as.character(test$user[u])
movieid = as.character(test$movie[u])
rating = rec_list[[userid]][movieid]
# 2.5 may be too arbitrary
test$rating[u] = ifelse(is.na(rating), 2.5, rating)
}
#Method 1 error
rmse = mean((test$rating - label$rating)^2)
rmse
#Method 1, UBCF
R = acast(train, user ~ movie)
#load libraries
rm(list = ls())
set.seed(9701)
library(dplyr)
library(ggplot2)
library(recommenderlab)
library(reshape2)
# ratings data
# use colClasses = 'NULL' to skip columns
ratings = read.csv('ratings.dat', sep = ':',
colClasses = c('integer', 'NULL'), header = FALSE)
colnames(ratings) = c('UserID', 'MovieID', 'Rating', 'Timestamp')
ratings$Timestamp = NULL;
colnames(ratings) = c('user', 'movie', 'rating')
train.id = sample(nrow(ratings), floor(nrow(ratings)) * 0.6)
train = ratings[train.id, ]
test = ratings[-train.id, ]
test.id = sample(nrow(test), floor(nrow(test)) * 0.5)
test = test[test.id, ]
label = test[c('user', 'rating')]
test$rating = NULL
#Method 1, UBCF
R = acast(train, user ~ movie)
R = as(R, 'realRatingMatrix')
rec = Recommender(R, method = 'UBCF',
parameter = list(normalize = 'Z-score', method = 'Cosine', nn = 5)
)
recom = predict(rec, R, type = 'ratings')
#Method 1, UBCF
R = acast(train, user ~ movie)
R = as(R, 'realRatingMatrix')
rec = Recommender(R, method = 'UBCF',
parameter = list(normalize = 'Z-score', method = 'Cosine', nn = 5)
)
recom = predict(rec, R, type = 'ratings')
rec_list = as(recom, 'list')
test$rating = NA
for (u in 1:nrow(test)){
# Read userid and movieid from columns 2 and 3 of test data
userid = as.character(test$user[u])
movieid = as.character(test$movie[u])
rating = rec_list[[userid]][movieid]
# 2.5 may be too arbitrary
test$rating[u] = ifelse(is.na(rating), 2.5, rating)
}
rmse = mean((test$rating - label$rating)^2)
rmse
start_time = Sys.time()
print(paste('Run time is:', Sys.time() - start_time))
print(paste('Run time is:', difftime(Sys.time(), start_time, units = 'mins'))
print(paste('Run time is:', difftime(Sys.time(), start_time, units = 'mins')))
difftime(Sys.time(), start_time, units = 'mins')
time_diff = difftime(Sys.time(), start_time, units = 'mins')
print(paste('Run time is:', time_diff))
print(paste('Run time is:', time_diff, 'mins'))
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
all = read.table("data.tsv",stringsAsFactors = F,header = T)
splits = read.table("splits.csv", header = T)
# remove HTML tags
all$review <- gsub('<.*?>', ' ', all$review)
all$review = gsub("[[:punct:][:blank:]]+", " ", all$review)
# library(textstem)
# all$review = lemmatize_strings(all$review)
# all$review = gsub('[[:digit:]]', ' ', all$review)
all$review = gsub('\\s+', ' ', all$review)
s = 1
train = all[-which(all$new_id%in%splits[,s]),]
test = all[which(all$new_id%in%splits[,s]),]
# YOUR CODE
# For example, start loading required libraries
set.seed(2017L)
library(text2vec)
library(glmnet)
library(magrittr)
library(slam)
library(pROC)
start_time = Sys.time()
# define preprocessing function and tokenization function
prep_fun = tolower
tok_fun = word_tokenizer
it = itoken(all$review,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = all$new_id,
progressbar = FALSE)
it_train = itoken(train$review,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = train$new_id,
progressbar = FALSE)
# Note that most text2vec functions are pipe friendly!
it_test = test$review %>%
prep_fun %>% tok_fun %>%
# turn off progressbar because it won't look nice in rmd
itoken(ids = test$new_id, progressbar = FALSE)
stop_words = c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours")
# stop_words = c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you",
#                "your", "yours", "yourself", "yourselves", "he", "him", "his",
#                "himself", "she", "her", "hers", "herself", "it", "its", "itself",
#                "they", "them", "their", "theirs", "themselves", "what", "which",
#                "who", "whom", "this", "that", "these", "those", "am", "is", "are",
#                "was", "were", "be", "been", "being", "have", "has", "had",
#                "having", "do", "does", "did", "doing", "a", "an", "the", "and",
#                "but", "if", "or", "because", "as", "until", "while", "of", "at",
#                "by", "for", "with", "about", "against", "between", "into",
#                "through", "during", "before", "after", "above", "below", "to",
#                "from", "up", "down", "in", "out", "on", "off", "over", "under",
#                "again", "further", "then", "once", "here", "there", "when",
#                "where", "why", "how", "all", "any", "both", "each", "few",
#                "more", "most", "other", "some", "such", "no", "nor", "not",
#                "only", "own", "same", "so", "than", "too", "very", "s", "t",
#                "can", "will", "just", "don", "should", "now")
# vocab = create_vocabulary(it, stopwords = stop_words)
vocab = create_vocabulary(it, ngram = c(1L, 2L), stopwords = stop_words)
pruned_vocab = prune_vocabulary(vocab,
term_count_min = 10,
doc_proportion_max = 0.5,
doc_proportion_min = 0.001)
vectorizer = vocab_vectorizer(pruned_vocab)
dtm_train = create_dtm(it_train, vectorizer)
identical(as.integer(rownames(dtm_train)), train$new_id)
dtm_test = create_dtm(it_test, vectorizer)
# # define tfidf model
# tfidf = TfIdf$new()
#
# # fit model to train data and transform train data with fitted model
# dtm_train = fit_transform(dtm_train, tfidf)
# dtm_test = transform(dtm_test, tfidf)
# v.size = dim(dtm_train)[2]
# ytrain = train$sentiment
# #summ[1, 1] is the mean frequency of the first vocab of positive sentiment across all documents
# #summ[1, 2] is the variance frequency of the first vocab of positive sentiment across all documents
# #summ[1, 3] is the mean frequency of the first vocab of negative sentiment across all documents
# #summ[1, 4] is the variance frequency of the first vocab of negative sentiment across all documents
# summ = matrix(0, nrow=v.size, ncol=4)
# summ[,1] = apply(dtm_train[ytrain==1, ], 2, mean)
# summ[,2] = apply(dtm_train[ytrain==1, ], 2, var)
# summ[,3] = apply(dtm_train[ytrain==0, ], 2, mean)
# summ[,4] = apply(dtm_train[ytrain==0, ], 2, var)
# n1=sum(ytrain);
# n=length(ytrain)
# n0= n - n1
# myp = (summ[,1] - summ[,3])/
#   sqrt(summ[,2]/n1 + summ[,4]/n0) #identical to words
words = colnames(dtm_train)
# id = order(abs(myp), decreasing=TRUE)[1:2000] #identical to part of words
id = read.table('my_vocab.txt', sep = ',', header = T)[, 1]
# vocab_list = cbind(id, words[id])
# colnames(vocab_list) = c('id', 'words')
# write.table(vocab_list, file = 'my_vocab.txt',sep = ',', row.names = F, quote = F)
# pos.list = words[id[myp[id]>0]]
# neg.list = words[id[myp[id]<0]]
#
# write.table(pos.list, file = 'pos_list.txt',sep = ',', row.names = F, quote = F)
# write.table(neg.list, file = 'neg_list.txt',sep = ',', row.names = F, quote = F)
set.seed(500)
NFOLDS = 10
mycv = cv.glmnet(x=dtm_train[, id], y=train$sentiment,
family='binomial',type.measure = "auc",
nfolds = NFOLDS, alpha=0)
myfit = glmnet(x=dtm_train[, id], y=train$sentiment,
lambda = mycv$lambda.min, family='binomial', alpha=0)
logit_pred = predict(myfit, dtm_test[, id], type = "response")
result = cbind(test$new_id, logit_pred)
colnames(result) = c('new_id', 'prob')
# write.table(result, file = 'Result_1.txt', sep = ',', row.names = F, quote = F)
write.table(result, file = 'mysubmission.txt', sep = ',', row.names = F, quote = F)
# my_roc <- roc(test$sentiment, as.vector(logit_pred))
# best_threshold = coords(my_roc, "best", ret = "threshold") #the best threshold
# pred_class = ifelse(logit_pred >= best_threshold, '1', '0')
# mis_classified = dtm_test[which(test$sentiment != pred_class), id]
# mis_id = sample(1:dim(mis_classified)[1], 5)
# test_mis = test[which(test$sentiment != pred_class), ]
# test_mis = test_mis[mis_id, ]
# mis_classified = mis_classified[mis_id, ]
# for (i in 1:5) {
#   v = mis_classified[i, ][which(mis_classified[i, ] != 0)]
#   filename = paste(i, '.txt')
#   write.table(v, filename)
# }
# write.table(test_mis, 'a.txt')
# nmis_classified = dtm_test[which(test$sentiment == pred_class), id]
#
# colnames(mis_classified)
#
# which(mis_classified[1, ] != 0)
#
# mean(mis_classified)
# mean(nmis_classified)
auc(test$sentiment, as.vector(logit_pred))
time_diff = difftime(Sys.time(), start_time, units = 'mins')
print(paste('Run time is:', time_diff, 'mins'))
load.packages('fOptions')
rm(list = ls())
library(quantmod)
library(MASS)
library(fOptions)
library(fExoticOptions)
filenames <- Sys.glob("*.csv")
mydata = list()
for (i in 1:length(filenames)) {
mydata[[i]] = read.csv(filenames[i])
}
n_stocks = length(mydata) #number of stocks
load.packages('fOptions')
rm(list = ls())
library(quantmod)
library(MASS)
library(fOptions)
library(fExoticOptions)
filenames <- Sys.glob("*.csv")
mydata = list()
for (i in 1:length(filenames)) {
mydata[[i]] = read.csv(filenames[i])
}
n_stocks = length(mydata) #number of stocks
length_stock = dim(mydata[[1]])[1] #The time length
log_returns = matrix(0, n_stocks, length_stock)
for (i in 1:n_stocks) {
log_returns[i, ] = Delt(mydata[[i]][, 2], type = 'log')[,1]
}
# get rid of the first column which is always na, 2*2994
log_returns = log_returns[, -1] #The ith, jth element is the jth return of the ith stock, ie. log(S_j/S_(j-1))
asset.paths <- function(s0, #the last day of the price
mu,
sigma,
nsims = 10000,
periods = 0:10   # time periods at which to simulate prices
)
{
s0 = as.vector(s0)
nsteps = length(periods)
dt = c(periods[1], diff(periods))
#One asset case
if( length(s0) == 1 ) {
drift = mu - 0.5 * sigma^2 #ie alpha in lemma
#period = 1
if( nsteps == 1 ) {
s0 * exp(drift * dt + sigma * sqrt(dt) * rnorm(nsims)) #S_0
} else { #period >1, if nsteps = 2, dt = c(0, 1)
temp = matrix(exp(drift * dt + sigma * sqrt(dt) * rnorm(nsteps * nsims)), nc=nsims) #2*10000, temp[1,1]:S_0/S_0, temp[2,1]:S_1/S_0
for(i in 2:nsteps) temp[i,] = temp[i,] * temp[(i-1),]
#temp[1,1]:S_0/S_0, temp[2,1]:S_1/S_0, if exists temp[3,1]:S_2/S_0
s0 * temp #temp[1,1]:S_0, temp[2,1]:S_1, temp[3, 1]:S_2
}
} else { #multi assets case
drift = mu - 0.5 * diag(sigma) #nstocks
n = length(mu) #nstocks
#period = 1
if( nsteps == 1 ) {
s0 * exp(drift * dt + sqrt(dt) * t(mvrnorm(nsims, rep(0, n), sigma)))
} else { #period >1
temp = array(exp(as.vector(drift %*% t(dt)) + #nstocks, period+1
t(sqrt(dt) * mvrnorm(nsteps * nsims, rep(0, n), sigma))), #
c(n, nsteps, nsims))
for(i in 2:nsteps) temp[,i,] = temp[,i,] * temp[,(i-1),] #temp[i,j,k] is the ith stock, jth period, kth simulation
s0 * temp
}
}
}
set.seed(1)
one_log_return = log_returns[1, ]
alpha = mean(one_log_return)
sigma_square = var(one_log_return)
mu = alpha + 0.5*sigma_square
S = mydata[[1]][length_stock, 2]
mu = mu
sigma = sqrt(sigma_square)
N = 10000
# Single Asset for 10 days
periods = 0:10
prices = asset.paths(S, mu, sigma, N, periods = periods)
# plot 100 simulations
matplot(prices[,1:100], type='l', xlab='Days', ylab='Prices',
main='Selected Price Paths')
set.seed(1)
alpha = rowMeans(log_returns) #nstocks
sigma_square = cov(t(log_returns))#nstocks, nstocks
mu = alpha + 0.5*diag(sigma_square)#nstocks
S = c() #the last day price
N = 10000
for (i in 1:n_stocks) {
S = c(S, mydata[[i]][length_stock, 2])
}
periods = 0:10
prices = asset.paths(S, mu, sigma_square, N, periods = periods) #[n, nsteps, nsims]
# plot
matplot(prices[1,,1:100], type='l', xlab='Years', ylab='Prices',
main='Selected Price Paths for Asset 1')
matplot(prices[2,,1:100], type='l', xlab='Years', ylab='Prices',
main='Selected Price Paths for Asset 2')
S
mu
sigma
sigma_square
sigma_square[1]
sigma_square[2]
sigma_square[3]
sigma_square[4]
#*****************************************************************
# Price European Call Option
#******************************************************************
load.packages('fOptions')
S = c(100,105)
X = 98
Time = 0.5
r = 0.05
sigma = c(0.11,0.16)
rho = 0.63
N = 10000
# Black–Scholes
GBSOption(TypeFlag = "c", S = S[1], X = X, Time = Time, r = r, b = r, sigma = sigma[1])
# Monte Carlo simulation
N = 1000000
prices = asset.paths(S[1], r, sigma[1], N, periods = Time)
dim(prices)
length(prices)
S = c(100,105)
X = 98
Time = 0.5
r = 0.05
sigma = c(0.11,0.16)
rho = 0.63
# Black–Scholes
GBSOption(TypeFlag = "c", S = S[1], X = X, Time = Time, r = r, b = r, sigma = sigma[1])
# Monte Carlo simulation
N = 1000
prices = asset.paths(S[1], r, sigma[1], N, periods = Time)
future.payoff = pmax(0, prices - X)
discounted.payoff = future.payoff * exp(-r * Time)
periods = 0.5
dt = c(periods[1], diff(periods))
dt
mydata[[1]]
