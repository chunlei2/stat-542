pred = predict(new_fit, newdata = testdata)
test_err = mean((pred - testdata$lpsa)^2)
test_err
new_fit = lm(lpsa ~ .-gleason, traindata)
pred = predict(new_fit, newdata = testdata)
test_err = mean((pred - testdata$lpsa)^2)
new = lm(lpsa ~ lcavol + lweight + lbph + svi, traindata)
pre = predict(new, newdata = testdata)
tes = mean((pre - testdata$lpsa)^2)
test_err
tes
names(traindata)
library(glmnet)  # glmnet for lasso
mylasso.cv = cv.glmnet(as.matrix(traindata[, -which(names(traindata) == 'lpsa')]), as.vector(traindata$lpsa))
plot(mylasso.cv)
mylasso.cv
tmp = predict(mylasso.cv, s="0.5",
newx=data.matrix(testdata)[, -which(names(traindata) == 'lpsa')])
library(glmnet)  # glmnet for lasso
mylasso.cv = cv.glmnet(as.matrix(traindata[, -which(names(traindata) == 'lpsa')]), as.vector(traindata$lpsa), alpha = 1, lambda = 0.5)
library(glmnet)  # glmnet for lasso
mylasso = glmnet(as.matrix(traindata[, -which(names(traindata) == 'lpsa')]), as.vector(traindata$lpsa), alpha = 1, lambda = 0.5)
tmp = predict(mylasso,
newx=data.matrix(testdata)[, -which(names(traindata) == 'lpsa')])
tmp = predict(mylasso,
newx=data.matrix(testdata)[, -which(names(testdata) == 'lpsa')])
mean((tmp - testdata$lpsa)^2)
mylasso = glmnet(as.matrix(traindata[, -which(names(traindata) == 'lpsa')]), as.vector(traindata$lpsa), alpha = 1, lambda = 0.1)
tmp = predict(mylasso,
newx=data.matrix(testdata)[, -which(names(testdata) == 'lpsa')])
mean((tmp - testdata$lpsa)^2)
mylasso = glmnet(as.matrix(traindata[, -which(names(traindata) == 'lpsa')]), as.vector(traindata$lpsa), alpha = 1, lambda = 0.01)
tmp = predict(mylasso,
newx=data.matrix(testdata)[, -which(names(testdata) == 'lpsa')])
mean((tmp - testdata$lpsa)^2)
mylasso
mylasso$call
library(MASS)
myData = Boston
names(myData)[14] = "Y"
iLog = c( 1,3,5,6,8,9,10,14 );
myData[,iLog] = log( myData[,iLog] );
myData[,2] = myData[,2] / 10;
myData[,7] = myData[,7]^2.5 / 10^4
myData[,11] = exp( 0.4 * myData[,11] ) / 1000;
myData[,12] = myData[,12] / 100;
myData[,13] = sqrt( myData[,13] );
library(MASS)
myData = Boston
names(myData)[14] = "Y"
iLog = c( 1,3,5,6,8,9,10,14 );
myData[,iLog] = log( myData[,iLog] );
myData[,2] = myData[,2] / 10;
myData[,7] = myData[,7]^2.5 / 10^4
myData[,11] = exp( 0.4 * myData[,11] ) / 1000;
myData[,12] = myData[,12] / 100;
myData[,13] = sqrt( myData[,13] );
full.model = lm( Y ~ ., data = myData);
stepAIC = step(full.model, direction="both", trace = 0)
stepAIC
myData$crim = 2*myData$crim
full.model = lm( Y ~ ., data = myData);
stepAIC = step(full.model, direction="both", trace = 0)
stepAIC
library(MASS)
myData = Boston
names(myData)[14] = "Y"
iLog = c( 1,3,5,6,8,9,10,14 );
myData[,iLog] = log( myData[,iLog] );
myData[,2] = myData[,2] / 10;
myData[,7] = myData[,7]^2.5 / 10^4
myData[,11] = exp( 0.4 * myData[,11] ) / 1000;
myData[,12] = myData[,12] / 100;
myData[,13] = sqrt( myData[,13] );
full.model = lm( Y ~ ., data = myData);
stepAIC = step(full.model, direction="both", trace = 0)
myData$crim
myData$crim = 2*myData$crim
full.model = lm( Y ~ ., data = myData);
stepAIC = step(full.model, direction="both", trace = 0)
myData$crim
install.packages('ElemStatLearn')
library("ElemStatLearn")
data("prostate")
prostate$train
traindata = prostate[prostate$train==TRUE,]
testdata = prostate[prostate$train==FALSE,]
traindata = within(traindata, rm(train))
testdata = within(testdata, rm(train))
myfit = lm(lpsa ~ . , data=traindata)
mypredict = predict(myfit, newdata=testdata)
sum((testdata$lpsa - mypredict)^2)/nrow(testdata)
library(leaps)
b = regsubsets(lpsa ~ ., data=traindata, nvmax = 8)
rs = summary(b)
rs$which
dim(traindata)
dim(testdata)
prostate$train
traindata = prostate[prostate$train==TRUE,]
testdata = prostate[prostate$train==FALSE,]
traindata = within(traindata, rm(train))
testdata = within(testdata, rm(train))
myfit = lm(lpsa ~ . , data=traindata)
mypredict = predict(myfit, newdata=testdata)
sum((testdata$lpsa - mypredict)^2)/nrow(testdata)
traindata = prostate[prostate$train==TRUE,]
testdata = prostate[prostate$train==FALSE,]
traindata = within(traindata, rm(train))
testdata = within(testdata, rm(train))
myfit = lm(lpsa ~ . , data=traindata)
mypredict = predict(myfit, newdata=testdata)
sum((testdata$lpsa - mypredict)^2)/nrow(testdata)
nrow(testdata)
traindata = prostate[prostate$train==TRUE,]
testdata = prostate[prostate$train==FALSE,]
traindata = within(traindata, rm(train))
testdata = within(testdata, rm(train))
myfit = lm(lpsa ~ . , data=traindata)
mypredict = predict(myfit, newdata=testdata)
mean((testdata$lpsa - mypredict)^2)
traindata = prostate[prostate$train==TRUE,]
testdata = prostate[prostate$train==FALSE,]
traindata = within(traindata, rm(train))
testdata = within(testdata, rm(train))
myfit = lm(lpsa ~ . , data=traindata)
mypredict = predict(myfit, newdata=testdata)
sum((testdata$lpsa - mypredict)^2)
library(leaps)
b = regsubsets(lpsa ~ ., data=traindata, method = 'exhaustive')
sumIC = summary(b)
msize = apply(sumIC$which, 1, sum)
AIC = n * log(sumIC$rss/n) + 2 * msize
BIC = n * log(sumIC$rss/n) + log(n) * msize
AIC; BIC
sumIC$rss
new_fit = lm(lpsa ~ .-gleason, traindata)
pred = predict(new_fit, newdata = testdata)
test_err = sum(pred - testdata$lpsa)^2
new = lm(lpsa ~ lcavol + lweight + lbph + svi, traindata)
pre = predict(new, newdata = testdata)
tes = sum((pre - testdata$lpsa)^2)
test_err
tes
library(leaps)
b = regsubsets(lpsa ~ ., data=traindata, method = 'exhaustive')
sumIC = summary(b)
msize = apply(sumIC$which, 1, sum)
AIC = n * log(sumIC$rss/n) + 2 * msize
BIC = n * log(sumIC$rss/n) + log(n) * msize
AIC; BIC
new_fit = lm(lpsa ~ .-gleason, traindata)
pred = predict(new_fit, newdata = testdata)
test_err = mean((pred - testdata$lpsa)^2)
new = lm(lpsa ~ lcavol + lweight + lbph + svi, traindata)
pre = predict(new, newdata = testdata)
tes = mean((pre - testdata$lpsa)^2)
test_err
tes
new_fit = lm(lpsa ~ .-gleason, traindata)
pred = predict(new_fit, newdata = testdata)
test_err = (pred - testdata$lpsa)^2
new = lm(lpsa ~ lcavol + lweight + lbph + svi, traindata)
pre = predict(new, newdata = testdata)
tes = (pre - testdata$lpsa)^2
test_err
new_fit = lm(lpsa ~ .-gleason, traindata)
pred = predict(new_fit, newdata = testdata)
test_err = sum((pred - testdata$lpsa)^2)
new = lm(lpsa ~ lcavol + lweight + lbph + svi, traindata)
pre = predict(new, newdata = testdata)
tes = sum((pre - testdata$lpsa)^2)
test_err
tes
library(leaps)
b = regsubsets(lpsa ~ ., data=traindata, method = 'exhaustive')
n = 30
sumIC = summary(b)
msize = apply(sumIC$which, 1, sum)
AIC = n * log(sumIC$rss/n) + 2 * msize
BIC = n * log(sumIC$rss/n) + log(n) * msize
AIC; BIC
AIC[which(AIC == min(AIC))]
BIC[which(BIC == min(BIC))]
library(leaps)
b = regsubsets(lpsa ~ ., data=traindata, method = 'exhaustive')
n = 67
sumIC = summary(b)
msize = apply(sumIC$which, 1, sum)
AIC = n * log(sumIC$rss/n) + 2 * msize
BIC = n * log(sumIC$rss/n) + log(n) * msize
AIC; BIC
AIC[which(AIC == min(AIC))]
BIC[which(BIC == min(BIC))]
sumIC
new_fit = lm(lpsa ~ .-gleason, traindata)
pred = predict(new_fit, newdata = testdata)
test_err = sum((pred - testdata$lpsa)^2)
new = lm(lpsa ~ lcavol + lweight, traindata)
pre = predict(new, newdata = testdata)
tes = sum((pre - testdata$lpsa)^2)
test_err
tes
U = runif(1000, 0, 1)
y = U^3 + 1.25*U^2 + 1/3*U + 1/6
plot(U, y)
U = runif(1000, 0, 1)
y = U^3 + 1.25*U^2 + 1/3*U + 1/6
plot(U, y)
U = runif(1000, 0, 1)
y = U^3 + 1.25*U^2 + 1/3*U + 1/6
plot(U, y)
U = runif(1000, 0, 1)
y = U^3 + 1.25*U^2 + 1/3*U + 1/6
plot(U, y)
U = runif(1000, 0, 1)
y = U^3 + 1.25*U^2 + 1/3*U + 1/6
plot(U, y, type = 'p')
density(y)
U = runif(1000, 0, 1)
y = U^3 + 1.25*U^2 + 1/3*U + 1/6
hist(y)
U = runif(1000, 0, 1)
y = U^3 + 1.25*U^2 + 1/3*U + 1/6
hist(y)
U = runif(1000, 0, 1)
y = U^3 + 1.25*U^2 + 1/3*U + 1/6
curve(x^3 + 5/4*x^2 + 1/3*x + 1/6, 0, 1)
U = runif(1000, 0, 1)
y = U^3 + 1.25*U^2 + 1/3*U + 1/6
curve(x^3 + 5/4*x^2 + 1/3*x + 1/6, 0, 1)
cureve(exp(-x), add = TRUE)
U = runif(1000, 0, 1)
y = U^3 + 1.25*U^2 + 1/3*U + 1/6
curve(x^3 + 5/4*x^2 + 1/3*x + 1/6, 0, 1)
curve(exp(-x), add = TRUE)
U = runif(1000, 0, 1)
y = U^3 + 1.25*U^2 + 1/3*U + 1/6
curve(x^3 + 5/4*x^2 + 1/3*x + 1/6, 0, 1)
curve(exp(x), add = TRUE)
dbeta(1,2)
dbeta(10, 1,2)
numeric(10)
1^3 + 1.25*1^2 + 1/3*1 + 1/6
n = 1000
k = 0
X = numeric(n)
while (k < n) {
u = runif(1)
x = runif(1)
if ((x^3 + 5/4*x^2 + 1/3*x + 1/6)/3 >= u) {
k = k + 1
X[k] = x
}
}
mean(X^2)
X
X^2
X
X^2
sqrt(var(X^2))
sqrt(1/1000*var(X^2))
getwd
getwd()
getwd()
set.seed(9301)
#number of predictors
p = 2
#sd for generating centers
s = 1
#number of observations in each class
n = 100
#number of centers
csize = 10
#generate twenty different 2-dim vectors from standard normal distribution
m1 = matrix(rnorm(csize*p), csize, p)*s + cbind(rep(1, csize), rep(0, csize))
m0 = matrix(rnorm(csize*p), csize, p)*s + cbind(rep(0, csize), rep(1, csize))
#choose 100 index of centers from each 10 different centers
id1 = sample(1:csize, n, replace = TRUE)
id0 = sample(1:csize, n, replace = TRUE)
#sd for generating x
s2 = sqrt(1/5)
#generate x from standard normal distribution
x_train = matrix(rnorm(2*n*p), 2*n, p)*s2 + rbind(m1[id1,], m0[id0,])
#generate y
y_train = c(rep(1,n), rep(0,n))
#generate test data
N = 5000
id1 = sample(1:csize, N, replace = TRUE)
id0 = sample(1:csize, N, replace = TRUE)
x_test = matrix(rnorm(2*N*p), 2*N, p)*s2 + rbind(m1[id1,], m0[id0,])
y_test = c(rep(1,N), rep(0,N))
#Linear regression
traindata = data.frame(x_train, y_train)
testdata = data.frame(x_test, y_test)
l_model = lm(y_train~ ., data = traindata)
Ytrain_pred_LS = as.numeric(l_model$fitted.values > 0.5)
Ytest_pred_LS = as.numeric((predict(l_model, testdata)) > 0.5)
##calculate train and test error
train_err_LS = sum(y_train != Ytrain_pred_LS)/(2*n)
test_err_LS = sum(y_test != Ytest_pred_LS)/(2*N)
#KNN
library(class)
myk = c(151, 101, 69, 45, 31, 21, 11, 7, 5, 3, 1)
m = length(myk)
train.err.knn = rep(0,m)
test.err.knn = rep(0, m)
y_train = factor(y_train)
y_test = factor(y_test)
for( j in 1:m){
Ytrain.pred = knn(x_train, x_train, y_train, k = myk[j])
train.err.knn[j] = sum(y_train != Ytrain.pred)/(2*n)
Ytest.pred = knn(x_train, x_test, y_train,k = myk[j])
test.err.knn[j] = sum(y_test != Ytest.pred)/(2*N)
}
#Bayes error
mixnorm=function(x){
## return the density ratio for a point x, where each
## density is a mixture of normal with 10 components
sum(exp(-apply((t(m1)-x)^2, 2, sum)*5/2))/sum(exp(-apply((t(m0)-x)^2, 2, sum)*5/2))
}
Ytest_pred_Bayes = apply(x_test, 1, mixnorm)
Ytest_pred_Bayes = as.numeric(Ytest_pred_Bayes > 1);
test.err.Bayes = sum(y_test !=  Ytest_pred_Bayes) / (2*N)
pdf("Assignment_1_9301_chunlei2_Chunlei.pdf")
plot(c(0.5,m), range(test_err_LS, train_err_LS, test.err.knn, train.err.knn),
type="n", xlab="Degree of Freedom", ylab="Error", xaxt="n")
df = round((2*n)/myk)
axis(1, at = 1:m, labels = df)
axis(3, at = 1:m, labels = myk)
points(1:m, test.err.knn, col="red", pch=1)
lines(1:m, test.err.knn, col="red", lty=1);
points(1:m, train.err.knn, col="blue", pch=1);
lines(1:m, train.err.knn, col="blue", lty=2);
points(3, train_err_LS, pch=2, cex=2, col="blue")
points(3, test_err_LS, pch=2, cex=2, col="red")
abline(test.err.Bayes, 0, col="purple")
legend("bottomleft", legend = c("Test_KNN", "Train_KNN", "Bayes", "Test_LS", "Train_LS"), lty = c(1,2,1,NA,NA), pch = c(NA,NA,NA,2,2), col = c("red", "blue", "purple", "red", "blue"))
dev.off()
getwd
getwd()
source("Untitled.R", chdir = T)
getScriptPath()
this.dir <- dirname(parent.frame(2)$ofile)
this.dir <- dirname(sys.frame(1)$ofile)
wd <- setwd(".")
setwd(wd)
getwd
getwd()
getwd()
getSrcDirectory(function(x) {x})
getwd()
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
setwd(dirname(rstudioapi::callFun("getActiveDocumentContext")$path))
rstudioapi::getSourceEditorContext()$path
getwd()
setwd(rstudioapi::getSourceEditorContext()$path)
dirname(rstudioapi::getSourceEditorContext()$path)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
getwd
getwd()
getwd(dirname(rstudioapi::getSourceEditorContext()$path))
set.seed(9301)
#number of predictors
p = 2
#sd for generating centers
s = 1
#number of observations in each class
n = 100
#number of centers
csize = 10
#generate twenty different 2-dim vectors from standard normal distribution
m1 = matrix(rnorm(csize*p), csize, p)*s + cbind(rep(1, csize), rep(0, csize))
m0 = matrix(rnorm(csize*p), csize, p)*s + cbind(rep(0, csize), rep(1, csize))
#choose 100 index of centers from each 10 different centers
id1 = sample(1:csize, n, replace = TRUE)
id0 = sample(1:csize, n, replace = TRUE)
#sd for generating x
s2 = sqrt(1/5)
#generate x from standard normal distribution
x_train = matrix(rnorm(2*n*p), 2*n, p)*s2 + rbind(m1[id1,], m0[id0,])
#generate y
y_train = c(rep(1,n), rep(0,n))
#generate test data
N = 5000
id1 = sample(1:csize, N, replace = TRUE)
id0 = sample(1:csize, N, replace = TRUE)
x_test = matrix(rnorm(2*N*p), 2*N, p)*s2 + rbind(m1[id1,], m0[id0,])
y_test = c(rep(1,N), rep(0,N))
#Linear regression
traindata = data.frame(x_train, y_train)
testdata = data.frame(x_test, y_test)
l_model = lm(y_train~ ., data = traindata)
Ytrain_pred_LS = as.numeric(l_model$fitted.values > 0.5)
Ytest_pred_LS = as.numeric((predict(l_model, testdata)) > 0.5)
##calculate train and test error
train_err_LS = sum(y_train != Ytrain_pred_LS)/(2*n)
test_err_LS = sum(y_test != Ytest_pred_LS)/(2*N)
#KNN
library(class)
myk = c(151, 101, 69, 45, 31, 21, 11, 7, 5, 3, 1)
m = length(myk)
train.err.knn = rep(0,m)
test.err.knn = rep(0, m)
y_train = factor(y_train)
y_test = factor(y_test)
for( j in 1:m){
Ytrain.pred = knn(x_train, x_train, y_train, k = myk[j])
train.err.knn[j] = sum(y_train != Ytrain.pred)/(2*n)
Ytest.pred = knn(x_train, x_test, y_train,k = myk[j])
test.err.knn[j] = sum(y_test != Ytest.pred)/(2*N)
}
#Bayes error
mixnorm=function(x){
## return the density ratio for a point x, where each
## density is a mixture of normal with 10 components
sum(exp(-apply((t(m1)-x)^2, 2, sum)*5/2))/sum(exp(-apply((t(m0)-x)^2, 2, sum)*5/2))
}
Ytest_pred_Bayes = apply(x_test, 1, mixnorm)
Ytest_pred_Bayes = as.numeric(Ytest_pred_Bayes > 1);
test.err.Bayes = sum(y_test !=  Ytest_pred_Bayes) / (2*N)
pdf("Assignment_1_9301_chunlei2_Chunlei.pdf")
plot(c(0.5,m), range(test_err_LS, train_err_LS, test.err.knn, train.err.knn),
type="n", xlab="Degree of Freedom", ylab="Error", xaxt="n")
df = round((2*n)/myk)
axis(1, at = 1:m, labels = df)
axis(3, at = 1:m, labels = myk)
points(1:m, test.err.knn, col="red", pch=1)
lines(1:m, test.err.knn, col="red", lty=1);
points(1:m, train.err.knn, col="blue", pch=1);
lines(1:m, train.err.knn, col="blue", lty=2);
points(3, train_err_LS, pch=2, cex=2, col="blue")
points(3, test_err_LS, pch=2, cex=2, col="red")
abline(test.err.Bayes, 0, col="purple")
legend("bottomleft", legend = c("Test_KNN", "Train_KNN", "Bayes", "Test_LS", "Train_LS"), lty = c(1,2,1,NA,NA), pch = c(NA,NA,NA,2,2), col = c("red", "blue", "purple", "red", "blue"))
dev.off()
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
set.seed(9301)
#number of predictors
p = 2
#sd for generating centers
s = 1
#number of observations in each class
n = 100
#number of centers
csize = 10
#generate twenty different 2-dim vectors from standard normal distribution
m1 = matrix(rnorm(csize*p), csize, p)*s + cbind(rep(1, csize), rep(0, csize))
m0 = matrix(rnorm(csize*p), csize, p)*s + cbind(rep(0, csize), rep(1, csize))
#choose 100 index of centers from each 10 different centers
id1 = sample(1:csize, n, replace = TRUE)
id0 = sample(1:csize, n, replace = TRUE)
#sd for generating x
s2 = sqrt(1/5)
#generate x from standard normal distribution
x_train = matrix(rnorm(2*n*p), 2*n, p)*s2 + rbind(m1[id1,], m0[id0,])
#generate y
y_train = c(rep(1,n), rep(0,n))
#generate test data
N = 5000
id1 = sample(1:csize, N, replace = TRUE)
id0 = sample(1:csize, N, replace = TRUE)
x_test = matrix(rnorm(2*N*p), 2*N, p)*s2 + rbind(m1[id1,], m0[id0,])
y_test = c(rep(1,N), rep(0,N))
#Linear regression
traindata = data.frame(x_train, y_train)
testdata = data.frame(x_test, y_test)
l_model = lm(y_train~ ., data = traindata)
Ytrain_pred_LS = as.numeric(l_model$fitted.values > 0.5)
Ytest_pred_LS = as.numeric((predict(l_model, testdata)) > 0.5)
##calculate train and test error
train_err_LS = sum(y_train != Ytrain_pred_LS)/(2*n)
test_err_LS = sum(y_test != Ytest_pred_LS)/(2*N)
#KNN
library(class)
myk = c(151, 101, 69, 45, 31, 21, 11, 7, 5, 3, 1)
m = length(myk)
train.err.knn = rep(0,m)
test.err.knn = rep(0, m)
y_train = factor(y_train)
y_test = factor(y_test)
for( j in 1:m){
Ytrain.pred = knn(x_train, x_train, y_train, k = myk[j])
train.err.knn[j] = sum(y_train != Ytrain.pred)/(2*n)
Ytest.pred = knn(x_train, x_test, y_train,k = myk[j])
test.err.knn[j] = sum(y_test != Ytest.pred)/(2*N)
}
#Bayes error
mixnorm=function(x){
## return the density ratio for a point x, where each
## density is a mixture of normal with 10 components
sum(exp(-apply((t(m1)-x)^2, 2, sum)*5/2))/sum(exp(-apply((t(m0)-x)^2, 2, sum)*5/2))
}
Ytest_pred_Bayes = apply(x_test, 1, mixnorm)
Ytest_pred_Bayes = as.numeric(Ytest_pred_Bayes > 1);
test.err.Bayes = sum(y_test !=  Ytest_pred_Bayes) / (2*N)
pdf("AssignmentOutput_1_9301_chunlei2_Chunlei.pdf")
plot(c(0.5,m), range(test_err_LS, train_err_LS, test.err.knn, train.err.knn),
type="n", xlab="Degree of Freedom", ylab="Error", xaxt="n")
df = round((2*n)/myk)
axis(1, at = 1:m, labels = df)
axis(3, at = 1:m, labels = myk)
points(1:m, test.err.knn, col="red", pch=1)
lines(1:m, test.err.knn, col="red", lty=1);
points(1:m, train.err.knn, col="blue", pch=1);
lines(1:m, train.err.knn, col="blue", lty=2);
points(3, train_err_LS, pch=2, cex=2, col="blue")
points(3, test_err_LS, pch=2, cex=2, col="red")
abline(test.err.Bayes, 0, col="purple")
legend("bottomleft", legend = c("Test_KNN", "Train_KNN", "Bayes", "Test_LS", "Train_LS"), lty = c(1,2,1,NA,NA), pch = c(NA,NA,NA,2,2), col = c("red", "blue", "purple", "red", "blue"))
dev.off()
4.31+2.19+2.29+2.79+10.43
